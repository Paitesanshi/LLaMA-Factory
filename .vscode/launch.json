{
    "version": "0.2.0",
    "configurations": [


        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "/home/v-leiwang8/LLaMA-Factory/src/train_bash.py",
            "console": "integratedTerminal",
            "args": [ "--stage",
                "sft",
                "--do_train",
                "--model_name_or_path",
                "THUDM/chatglm3-6b",
                "--dataset",
                "reward_data",
                "--dataset_dir",
                "data",
                "--template",
                "default",
                "--finetuning_type",
                "lora",
                "--lora_target",
                "query_key_value",
                "--output_dir",
                "saves/ChatGLM/lora/point",
                "--overwrite_cache",
                "--overwrite_output_dir",
                "--cutoff_len",
                "1024",
                "--preprocessing_num_workers",
                "16",
                "--per_device_train_batch_size",
                "1",
                "--per_device_eval_batch_size",
                "1",
                "--gradient_accumulation_steps",
                "8",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "10",
                "--warmup_steps",
                "20",
                "--save_steps",
                "100",
                "--eval_steps",
                "100",
                "--evaluation_strategy",
                "steps",
                "--load_best_model_at_end",
                "--learning_rate",
                "5e-5",
                "--num_train_epochs",
                "3.0",
                "--max_samples",
                "3000",
                "--val_size",
                "0.1",
                "--plot_loss",
                "--fp16"]
        }
    ]
}